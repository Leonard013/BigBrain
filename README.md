# ğŸ§  BigBrain

MCP server that lets Claude Code talk to Codex (OpenAI) and Gemini (Google) CLIs. Claude stays in charge â€” the other models provide second opinions. No API keys needed; both CLIs use their own cached OAuth. *Free as in beer* ğŸº

Inspired by [Karpathy's llm-council](https://github.com/karpathy/llm-council).

## ğŸ¹ Origin Story

> *This project was born on a Saturday night vibecoding session fueled by spiced rum.*

The kind of evening where you look at your API bill ğŸ’¸, look at your glass ğŸ¥ƒ, and think: ***"there has to be a better way."***

Turns out, both Codex and Gemini have CLIs with free OAuth access. So instead of paying per token like a responsible adult, BigBrain just shells out to the CLIs like a genius cheapskate. No API keys. No billing surprises on Monday morning. Just three AI models arguing with each other over stdio while you sip your drink. ğŸ¹

Does it work? **Yes.** Is it elegant? *Debatable.* Did it save money? **Absolutely.** ğŸ’° Will your future self thank your rum-fueled past self? *Also debatable.* ğŸ˜…

> ***Enjoy responsibly*** *(the rum and the code).* ğŸ¥‚

## ğŸ—ï¸ Architecture

```
You ğŸ«µ
 â””â”€> Claude Code (chairman â€” decides, writes memory, synthesizes)
       â””â”€> BigBrain MCP Server (Python, FastMCP, stdio)
             â”œâ”€> Codex CLI  (async subprocess)
             â””â”€> Gemini CLI (async subprocess)
```

**ğŸ”— Shared context**: Every prompt sent to Codex/Gemini is automatically prepended with the current project's `CLAUDE.md` and `MEMORY.md`. Claude is the sole writer of these files; the other models only read them.

**ğŸ“ Dynamic project detection**: All tools accept an optional `project_path` param. Falls back to `BIGBRAIN_PROJECT_PATH` env var, then cwd. Works with any Claude Code project.

## ğŸ› ï¸ Tools

| Tool | What it does |
|------|-------------|
| `ask_codex` | Ask Codex a single question ğŸ¤– |
| `ask_gemini` | Ask Gemini a single question ğŸ’ |
| `ask_both_models` | Ask both in parallel, get both answers âš¡ |
| `request_consensus` | Both answer independently, then one synthesizes agreements/differences ğŸ¤ |
| `request_debate` | Multi-round debate â€” each model sees the other's previous answer (1â€“5 rounds) ğŸ¥Š |
| `request_council` | Three-stage council with anonymized peer review *(see below)* ğŸ‘‘ |

## ğŸ‘‘ Council â€” Three-Stage Process

The most thorough tool. All three models participate equally. Based on [Karpathy's llm-council](https://github.com/karpathy/llm-council):

```
Stage 1: Individual ğŸ—£ï¸       Stage 2: Peer Review ğŸ”      Stage 3: Chairman ğŸ›ï¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude  â”‚â”€â”€> Answer A      â”‚ Codex   â”‚â”€â”€> Reviews &     â”‚ Claude  â”‚â”€â”€> Final
â”‚ Codex   â”‚â”€â”€> Answer B      â”‚ Gemini  â”‚    ranks all     â”‚         â”‚   answer
â”‚ Gemini  â”‚â”€â”€> Answer C      â”‚ Claude  â”‚    (anonymized)  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  (all three)                  (all three)                 (chairman)
```

1. **ğŸ—£ï¸ Individual** â€” Claude forms its opinion first, then Codex and Gemini answer independently in parallel. All three answers collected.
2. **ğŸ” Peer Review** â€” All answers are anonymized as Model A/B/C. Codex and Gemini each review and rank them. Claude receives the same anonymized prompt and does its own review too. *No model knows which answer is whose* ğŸ•µï¸
3. **ğŸ›ï¸ Chairman** â€” Claude synthesizes all three individual answers + all three peer reviews into the final answer.

## ğŸš€ Setup

### 1. Install CLIs

```bash
npm install -g @openai/codex @google/gemini-cli
```

Authenticate each (one-time):

```bash
codex   # follow OAuth prompts ğŸ”
gemini  # follow OAuth prompts ğŸ”
```

### 2. Create conda environment ğŸ

```bash
conda create -n bigbrain python=3.12 -y
```

### 3. Install BigBrain

```bash
/path/to/miniconda3/envs/bigbrain/bin/pip install -e /path/to/BigBrain
```

### 4. Register with Claude Code

Add to `~/.claude.json` under `mcpServers`:

```json
{
  "bigbrain": {
    "command": "/path/to/BigBrain/run_server.sh",
    "args": []
  }
}
```

Update `run_server.sh` to point to your conda python:

```bash
#!/usr/bin/env bash
exec /path/to/miniconda3/envs/bigbrain/bin/python -m bigbrain.server "$@"
```

### 5. Restart Claude Code ğŸ”„

Start a new session. Run `/mcp` â€” `bigbrain` should show as connected. âœ…

## âš™ï¸ Configuration

Models default to `gpt-5.3-codex` and `gemini-3-pro-preview`. Override via env vars:

```bash
export BIGBRAIN_CODEX_MODEL="gpt-5.3-codex"
export BIGBRAIN_GEMINI_MODEL="gemini-3-pro-preview"
```

## ğŸ“ Project Structure

```
src/bigbrain/
  server.py        # FastMCP app â€” all 6 MCP tools
  config.py        # Paths, timeouts, model names
  context.py       # CLAUDE.md + MEMORY.md reader
  models/
    base.py        # Abstract CLIModelAdapter (async subprocess)
    codex.py       # Codex CLI adapter
    gemini.py      # Gemini CLI adapter
  orchestrator.py  # Parallel, consensus, debate, council patterns
```

## ğŸ§ª Dev

```bash
# run tests
/path/to/miniconda3/envs/bigbrain/bin/pytest tests/ -v

# reinstall after changes
/path/to/miniconda3/envs/bigbrain/bin/pip install -e .
```
